"""
å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
æ”¯æŒå¤šç§LLMæ¥å£ï¼ŒåŒ…æ‹¬OpenAIã€vLLMç­‰
"""
import os
import json
import requests
from typing import Optional, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMClient:
    """
    å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ç±»
    
    è®¾è®¡ç†å¿µï¼š
    1. ç»Ÿä¸€æ¥å£ï¼šä¸åŒLLMæä¾›å•†ä½¿ç”¨ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
    2. å¯é…ç½®æ€§ï¼šæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‚æ•°é…ç½®ä¸åŒçš„æ¨¡å‹
    3. é”™è¯¯å¤„ç†ï¼šåŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    4. æ‰©å±•æ€§ï¼šæ˜“äºæ·»åŠ æ–°çš„LLMæä¾›å•†æ”¯æŒ
    """
    
    def __init__(self, 
                 model_type: str = "openai",
                 model_name: str = "gpt-3.5-turbo",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 temperature: float = 0.7):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ (openai, vllm, demo)
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: è‡ªå®šä¹‰APIåœ°å€
            temperature: ç”Ÿæˆæ¸©åº¦å‚æ•°
        """
        self.model_type = model_type
        self.model_name = model_name
        self.temperature = temperature
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–é…ç½®
        if model_type == "openai":
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            self.base_url = base_url or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        elif model_type == "vllm":
            self.api_key = "dummy"  # vLLMé€šå¸¸ä¸éœ€è¦å¯†é’¥
            self.base_url = base_url or os.getenv("VLLM_BASE_URL", "http://localhost:8000")
        elif model_type == "demo":
            self.api_key = "demo"
            self.base_url = "demo"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    def generate_text(self, prompt: str, max_tokens: int = 1000) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        """
        if self.model_type == "openai":
            return self._call_openai(prompt, max_tokens)
        elif self.model_type == "vllm":
            return self._call_vllm(prompt, max_tokens)
        elif self.model_type == "demo":
            return self._call_demo(prompt, max_tokens)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _call_openai(self, prompt: str, max_tokens: int) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            import openai
            
            # æ£€æŸ¥APIå¯†é’¥
            if not self.api_key or self.api_key in ["your_openai_api_key_here", "demo"]:
                raise Exception("è¯·é…ç½®æœ‰æ•ˆçš„OpenAI APIå¯†é’¥")
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            # å‘é€è¯·æ±‚
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ¥å‘Šæ’°å†™åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡ã€è¯¦ç»†çš„å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=self.temperature
            )
            
            return response.choices[0].message.content.strip()
            
        except ImportError:
            raise Exception("è¯·å®‰è£…openaiåº“: pip install openai")
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            # å¦‚æœæ˜¯APIé…ç½®é—®é¢˜ï¼Œæä¾›å‹å¥½æç¤º
            if "api" in str(e).lower() or "key" in str(e).lower():
                return "âš ï¸ OpenAI APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚è¿è¡Œ `python configure_llm.py` è¿›è¡Œé…ç½®ã€‚"
            return self._call_demo(prompt, max_tokens)  # é™çº§åˆ°æ¼”ç¤ºæ¨¡å¼
    
    def _call_vllm(self, prompt: str, max_tokens: int) -> str:
        """è°ƒç”¨vLLM API"""
        try:
            # æ„å»ºè¯·æ±‚URL
            url = f"{self.base_url.rstrip('/')}/v1/chat/completions"
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ¥å‘Šæ’°å†™åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡ã€è¯¦ç»†çš„å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": self.temperature
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(url, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
            
        except requests.exceptions.ConnectionError:
            print(f"vLLMè¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ° {self.base_url}")
            return "âš ï¸ vLLMæœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨ã€‚è¿è¡Œ `python configure_llm.py` è¿›è¡Œé…ç½®ã€‚"
        except Exception as e:
            print(f"vLLM APIè°ƒç”¨å¤±è´¥: {e}")
            return self._call_demo(prompt, max_tokens)  # é™çº§åˆ°æ¼”ç¤ºæ¨¡å¼
    
    def _call_demo(self, prompt: str, max_tokens: int) -> str:
        """æ¼”ç¤ºæ¨¡å¼ - ç”Ÿæˆç¤ºä¾‹å†…å®¹"""
        # æ ¹æ®promptç±»å‹ç”Ÿæˆä¸åŒçš„æ¼”ç¤ºå†…å®¹
        if "å¤§çº²" in prompt or "outline" in prompt.lower():
            return """1. æ¦‚è¿°ä¸èƒŒæ™¯
2. ç°çŠ¶åˆ†æ
3. æ ¸å¿ƒæŠ€æœ¯/æ–¹æ³•
4. åº”ç”¨æ¡ˆä¾‹
5. å‘å±•è¶‹åŠ¿
6. æŒ‘æˆ˜ä¸æœºé‡
7. ç»“è®ºä¸å»ºè®®"""
        
        elif "æ¶¦è‰²" in prompt or "polish" in prompt.lower():
            # å¦‚æœæ˜¯æ¶¦è‰²è¯·æ±‚ï¼Œè¿”å›ç¨å¾®æ”¹è¿›çš„å†…å®¹
            if len(prompt) > 100:
                return prompt.replace("è¿™æ˜¯ä¸€ä¸ª", "è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„").replace("éœ€è¦", "äºŸéœ€").replace("ã€‚", "ï¼Œä¸ºè¡Œä¸šå‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚")
            
        elif "å›¾è¡¨" in prompt or "chart" in prompt.lower():
            return """å»ºè®®ç”Ÿæˆä»¥ä¸‹å›¾è¡¨ï¼š
1. å‘å±•è¶‹åŠ¿å›¾ - å±•ç¤ºæŠ€æœ¯å‘å±•å†ç¨‹
2. å¯¹æ¯”åˆ†æå›¾ - ä¸åŒæ–¹æ¡ˆçš„ä¼˜åŠ£å¯¹æ¯”
3. å¸‚åœºä»½é¢å›¾ - å„å‚å•†å¸‚åœºå æ¯”
4. æµç¨‹å›¾ - æ ¸å¿ƒæŠ€æœ¯å®ç°æµç¨‹"""
        
        # é»˜è®¤å†…å®¹ç”Ÿæˆ
        topic = prompt.split("ï¼š")[-1] if "ï¼š" in prompt else prompt
        return f"""
{topic}æ˜¯å½“å‰é‡è¦çš„ç ”ç©¶é¢†åŸŸå’Œå‘å±•æ–¹å‘ã€‚

åœ¨æŠ€æœ¯å±‚é¢ï¼Œ{topic}å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- åˆ›æ–°æ€§å¼ºï¼ŒæŠ€æœ¯æ¶æ„å…ˆè¿›
- åº”ç”¨åœºæ™¯å¹¿æ³›ï¼Œå¸‚åœºå‰æ™¯è‰¯å¥½  
- å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œå…¼å®¹æ€§
- å®‰å…¨æ€§å’Œç¨³å®šæ€§ä¸æ–­æå‡

ä»å‘å±•ç°çŠ¶æ¥çœ‹ï¼Œ{topic}æ­£å¤„äºå¿«é€Ÿå‘å±•æœŸï¼Œä¸»è¦è¡¨ç°åœ¨ï¼š
- æŠ€æœ¯æˆç†Ÿåº¦ä¸æ–­æå‡
- äº§ä¸šç”Ÿæ€æ—¥è¶‹å®Œå–„
- æ ‡å‡†åŒ–ç¨‹åº¦é€æ­¥æé«˜
- åº”ç”¨é¢†åŸŸæŒç»­æ‰©å¤§

é¢å‘æœªæ¥ï¼Œ{topic}çš„å‘å±•è¶‹åŠ¿åŒ…æ‹¬ï¼š
- æŠ€æœ¯èåˆç¨‹åº¦è¿›ä¸€æ­¥åŠ æ·±
- åº”ç”¨åœºæ™¯æ›´åŠ ä¸°å¯Œå¤šæ ·
- äº§ä¸šè§„æ¨¡æŒç»­æ‰©å¤§
- å›½é™…åˆä½œä¸æ–­åŠ å¼º

æ€»ä½“è€Œè¨€ï¼Œ{topic}å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯å’Œé‡è¦çš„æˆ˜ç•¥ä»·å€¼ï¼Œå€¼å¾—æŒç»­å…³æ³¨å’Œæ·±å…¥ç ”ç©¶ã€‚

ğŸ”¸ æ³¨æ„ï¼šè¿™æ˜¯æ¼”ç¤ºå†…å®¹ï¼Œå¦‚éœ€è·å¾—çœŸå®AIç”Ÿæˆçš„å†…å®¹ï¼Œè¯·é…ç½®APIå¯†é’¥ã€‚
""".strip()
    
    def batch_generate(self, prompts: list, max_tokens: int = 1000) -> list:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
        """
        results = []
        for prompt in prompts:
            result = self.generate_text(prompt, max_tokens)
            results.append(result)
        return results
