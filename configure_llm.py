#!/usr/bin/env python3
"""
å¤§æ¨¡å‹é…ç½®åŠ©æ‰‹
å¸®åŠ©ç”¨æˆ·å¿«é€Ÿé…ç½®OpenAIã€vLLMç­‰å¤§æ¨¡å‹æœåŠ¡
"""
import os
import sys
import json
from pathlib import Path

def main():
    print("ğŸ”§ å¤šæ™ºèƒ½ä½“æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ - å¤§æ¨¡å‹é…ç½®åŠ©æ‰‹")
    print("=" * 60)
    
    # æ£€æŸ¥å½“å‰é…ç½®
    env_file = Path('.env')
    has_config = env_file.exists()
    
    if has_config:
        print("âœ… å‘ç°ç°æœ‰é…ç½®æ–‡ä»¶ .env")
        with open(env_file, 'r', encoding='utf-8') as f:
            content = f.read()
            if 'OPENAI_API_KEY' in content and content.split('OPENAI_API_KEY=')[1].split('\n')[0].strip() != 'your_openai_api_key_here':
                print("âœ… OpenAI APIå¯†é’¥å·²é…ç½®")
            else:
                print("âš ï¸  OpenAI APIå¯†é’¥æœªé…ç½®")
    else:
        print("âš ï¸  æœªå‘ç°é…ç½®æ–‡ä»¶")
    
    print("\nğŸ“‹ è¯·é€‰æ‹©é…ç½®æ–¹å¼:")
    print("1. é…ç½® OpenAI API (æ¨è)")
    print("2. é…ç½®æœ¬åœ° vLLM æœåŠ¡")
    print("3. ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ (æ— éœ€APIå¯†é’¥)")
    print("4. æµ‹è¯•å½“å‰é…ç½®")
    print("5. é€€å‡º")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
    
    if choice == '1':
        configure_openai()
    elif choice == '2':
        configure_vllm()
    elif choice == '3':
        configure_demo()
    elif choice == '4':
        test_configuration()
    elif choice == '5':
        print("ğŸ‘‹ é…ç½®å·²å–æ¶ˆ")
        sys.exit(0)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        main()

def configure_openai():
    """é…ç½®OpenAI API"""
    print("\nğŸ¤– é…ç½® OpenAI API")
    print("-" * 30)
    
    print("ğŸ“ OpenAI APIé…ç½®è¯´æ˜:")
    print("1. è®¿é—® https://platform.openai.com/api-keys")
    print("2. åˆ›å»ºæ–°çš„APIå¯†é’¥")
    print("3. å¤åˆ¶å¯†é’¥å¹¶ç²˜è´´åˆ°ä¸‹é¢")
    print("4. ç¡®ä¿è´¦æˆ·æœ‰è¶³å¤Ÿä½™é¢")
    
    api_key = input("\nè¯·è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥: ").strip()
    
    if not api_key or api_key == 'your_openai_api_key_here':
        print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        return configure_openai()
    
    # å¯é€‰ï¼šè‡ªå®šä¹‰base URL
    print("\nğŸŒ API Base URLé…ç½® (å¯é€‰)")
    print("1. ä½¿ç”¨å®˜æ–¹åœ°å€ (é»˜è®¤)")
    print("2. ä½¿ç”¨ä»£ç†åœ°å€")
    
    url_choice = input("è¯·é€‰æ‹© (1-2, é»˜è®¤1): ").strip() or "1"
    
    if url_choice == "2":
        base_url = input("è¯·è¾“å…¥ä»£ç†åœ°å€ (å¦‚: https://api.chatanywhere.tech): ").strip()
    else:
        base_url = "https://api.openai.com/v1"
    
    # æ¨¡å‹é€‰æ‹©
    print("\nğŸ¯ æ¨¡å‹é€‰æ‹©:")
    models = [
        ("gpt-3.5-turbo", "GPT-3.5 Turbo (æ¨èï¼Œæ€§ä»·æ¯”é«˜)"),
        ("gpt-4", "GPT-4 (è´¨é‡æœ€ä½³ï¼Œæˆæœ¬è¾ƒé«˜)"),
        ("gpt-4-turbo", "GPT-4 Turbo (å¹³è¡¡é€‰æ‹©)"),
        ("gpt-4o", "GPT-4o (æœ€æ–°æ¨¡å‹)")
    ]
    
    for i, (model, desc) in enumerate(models, 1):
        print(f"{i}. {desc}")
    
    model_choice = input("è¯·é€‰æ‹©æ¨¡å‹ (1-4, é»˜è®¤1): ").strip() or "1"
    selected_model = models[int(model_choice) - 1][0]
    
    # å†™å…¥é…ç½®æ–‡ä»¶
    write_env_config({
        'OPENAI_API_KEY': api_key,
        'OPENAI_BASE_URL': base_url,
        'DEFAULT_MODEL_TYPE': 'openai',
        'DEFAULT_MODEL_NAME': selected_model
    })
    
    print(f"\nâœ… OpenAIé…ç½®å®Œæˆ!")
    print(f"ğŸ“‹ æ¨¡å‹: {selected_model}")
    print(f"ğŸŒ Base URL: {base_url}")
    
    # æµ‹è¯•é…ç½®
    if input("\nğŸ§ª æ˜¯å¦æµ‹è¯•é…ç½®? (y/N): ").lower() == 'y':
        test_openai_connection(api_key, base_url, selected_model)

def configure_vllm():
    """é…ç½®æœ¬åœ°vLLMæœåŠ¡"""
    print("\nğŸ–¥ï¸  é…ç½®æœ¬åœ° vLLM æœåŠ¡")
    print("-" * 30)
    
    print("ğŸ“ vLLMé…ç½®è¯´æ˜:")
    print("1. ç¡®ä¿å·²å®‰è£…vLLM: pip install vllm")
    print("2. å¯åŠ¨vLLMæœåŠ¡å™¨")
    print("3. é»˜è®¤åœ°å€é€šå¸¸ä¸º: http://localhost:8000")
    print("")
    print("ğŸ’¡ æç¤º:")
    print("- å¦‚æœè¿˜æœªå®‰è£…vLLMï¼Œå¯ä»¥è¿è¡Œ: ./setup_vllm.sh")
    print("- è¯¦ç»†é…ç½®æŒ‡å—è¯·æŸ¥çœ‹: VLLM_SETUP_GUIDE.md")
    print("")
    
    # æ£€æŸ¥æœ¬åœ°DeepSeekæ¨¡å‹
    deepseek_path = "/home/scp/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562"
    deepseek_available = os.path.exists(f"{deepseek_path}/config.json")
    
    if deepseek_available:
        print("âœ… æ£€æµ‹åˆ°æœ¬åœ°DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {deepseek_path}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰vLLMæœåŠ¡è¿è¡Œ
    try:
        import requests
        response = requests.get("http://localhost:8000/v1/models", timeout=5)
        if response.status_code == 200:
            models = response.json()
            print("âœ… æ£€æµ‹åˆ°è¿è¡Œä¸­çš„vLLMæœåŠ¡")
            if 'data' in models and models['data']:
                available_models = [model['id'] for model in models['data']]
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
                default_model = available_models[0] if available_models else ("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" if deepseek_available else "Qwen/Qwen2-7B-Instruct")
            else:
                default_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" if deepseek_available else "Qwen/Qwen2-7B-Instruct"
        else:
            print("âš ï¸  vLLMæœåŠ¡æœªè¿è¡Œæˆ–å“åº”å¼‚å¸¸")
            default_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" if deepseek_available else "Qwen/Qwen2-7B-Instruct"
    except:
        print("âš ï¸  vLLMæœåŠ¡æœªè¿è¡Œ")
        default_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" if deepseek_available else "Qwen/Qwen2-7B-Instruct"
    
    print(f"\nğŸ”§ é…ç½®vLLMè¿æ¥")
    
    # æ¨¡å‹é€‰æ‹©
    print(f"\nğŸ¤– å¯ç”¨æ¨¡å‹é€‰é¡¹:")
    print("1. deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (æœ¬åœ°å·²ä¸‹è½½)" if deepseek_available else "1. deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (éœ€è¦ä¸‹è½½)")
    print("2. Qwen/Qwen2-7B-Instruct")
    print("3. Qwen/Qwen2-1.5B-Instruct")
    print("4. THUDM/chatglm3-6b")
    print("5. è‡ªå®šä¹‰æ¨¡å‹")
    
    model_choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (1-5, é»˜è®¤ä½¿ç”¨ {default_model}): ").strip()
    
    if model_choice == "1":
        model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    elif model_choice == "2":
        model_name = "Qwen/Qwen2-7B-Instruct"
    elif model_choice == "3":
        model_name = "Qwen/Qwen2-1.5B-Instruct"
    elif model_choice == "4":
        model_name = "THUDM/chatglm3-6b"
    elif model_choice == "5":
        model_name = input("è¯·è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°: ").strip()
    else:
        model_name = default_model if model_choice == "" else default_model
    
    base_url = input(f"è¯·è¾“å…¥vLLMæœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:8000): ").strip()
    if not base_url:
        base_url = "http://localhost:8000"
    
    # é«˜çº§é…ç½®é€‰é¡¹
    print(f"\nâš™ï¸  é«˜çº§é…ç½® (å¯é€‰)")
    advanced = input("æ˜¯å¦é…ç½®é«˜çº§é€‰é¡¹? (y/N): ").lower() == 'y'
    
    config = {
        'VLLM_BASE_URL': base_url,
        'DEFAULT_MODEL_TYPE': 'vllm',
        'DEFAULT_MODEL_NAME': model_name
    }
    
    if advanced:
        temperature = input("æ¸©åº¦å‚æ•° (0.1-2.0, é»˜è®¤0.7): ").strip()
        if temperature:
            try:
                temp_val = float(temperature)
                if 0.1 <= temp_val <= 2.0:
                    config['DEFAULT_TEMPERATURE'] = temperature
            except ValueError:
                pass
        
        max_tokens = input("æœ€å¤§ç”Ÿæˆé•¿åº¦ (é»˜è®¤1000): ").strip()
        if max_tokens:
            try:
                tokens_val = int(max_tokens)
                if tokens_val > 0:
                    config['DEFAULT_MAX_TOKENS'] = max_tokens
            except ValueError:
                pass
    
    # å†™å…¥é…ç½®æ–‡ä»¶
    write_env_config(config)
    
    print(f"\nâœ… vLLMé…ç½®å®Œæˆ!")
    print(f"ğŸŒ æœåŠ¡åœ°å€: {base_url}")
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    
    # æµ‹è¯•é…ç½®
    if input("\nğŸ§ª æ˜¯å¦æµ‹è¯•é…ç½®? (y/N): ").lower() == 'y':
        test_vllm_connection(base_url, model_name)
        
    # æä¾›å¯åŠ¨æŒ‡å—
    print(f"\nğŸ“‹ vLLMæœåŠ¡å¯åŠ¨æŒ‡å—:")
    print(f"å¦‚æœvLLMæœåŠ¡æœªè¿è¡Œï¼Œè¯·æ‰§è¡Œ:")
    print(f"./start_vllm.sh")
    print(f"æˆ–æ‰‹åŠ¨å¯åŠ¨:")
    print(f"python -m vllm.entrypoints.openai.api_server --model {model_name} --host 0.0.0.0 --port 8000")

def configure_demo():
    """é…ç½®æ¼”ç¤ºæ¨¡å¼"""
    print("\nğŸ­ é…ç½®æ¼”ç¤ºæ¨¡å¼")
    print("-" * 30)
    
    print("ğŸ“ æ¼”ç¤ºæ¨¡å¼è¯´æ˜:")
    print("âœ… æ— éœ€APIå¯†é’¥")
    print("âœ… å¯ä»¥ä½“éªŒå®Œæ•´æµç¨‹")
    print("âš ï¸  ç”Ÿæˆçš„å†…å®¹ä¸ºæ¼”ç¤ºå†…å®¹")
    print("âš ï¸  ä¸è°ƒç”¨çœŸå®AIæ¨¡å‹")
    
    # å†™å…¥é…ç½®æ–‡ä»¶
    write_env_config({
        'DEFAULT_MODEL_TYPE': 'demo',
        'DEFAULT_MODEL_NAME': 'demo-model'
    })
    
    print("\nâœ… æ¼”ç¤ºæ¨¡å¼é…ç½®å®Œæˆ!")
    print("ğŸ¯ ç°åœ¨å¯ä»¥ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ç”ŸæˆæŠ¥å‘Š")

def test_configuration():
    """æµ‹è¯•å½“å‰é…ç½®"""
    print("\nğŸ§ª æµ‹è¯•å½“å‰é…ç½®")
    print("-" * 30)
    
    # è¯»å–é…ç½®
    env_file = Path('.env')
    if not env_file.exists():
        print("âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ .env")
        return
    
    config = {}
    with open(env_file, 'r', encoding='utf-8') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                config[key] = value
    
    model_type = config.get('DEFAULT_MODEL_TYPE', '')
    
    if model_type == 'openai':
        api_key = config.get('OPENAI_API_KEY', '')
        base_url = config.get('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        model_name = config.get('DEFAULT_MODEL_NAME', 'gpt-3.5-turbo')
        test_openai_connection(api_key, base_url, model_name)
    elif model_type == 'vllm':
        base_url = config.get('VLLM_BASE_URL', 'http://localhost:8000')
        test_vllm_connection(base_url)
    elif model_type == 'demo':
        print("âœ… æ¼”ç¤ºæ¨¡å¼é…ç½®æ­£å¸¸")
        print("ğŸ¯ å¯ä»¥ç›´æ¥ä½¿ç”¨Webç•Œé¢æˆ–å‘½ä»¤è¡Œå·¥å…·")
    else:
        print("âŒ æœªé…ç½®æ¨¡å‹ç±»å‹")

def test_openai_connection(api_key, base_url, model_name):
    """æµ‹è¯•OpenAIè¿æ¥"""
    print(f"\nğŸ” æµ‹è¯•OpenAIè¿æ¥...")
    print(f"ğŸŒ Base URL: {base_url}")
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    
    try:
        import openai
        
        client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10
        )
        
        print("âœ… OpenAIè¿æ¥æµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“ å“åº”: {response.choices[0].message.content}")
        
    except Exception as e:
        print(f"âŒ OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   - APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        print("   - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("   - è´¦æˆ·ä½™é¢æ˜¯å¦å……è¶³")

def test_vllm_connection(base_url, model_name=None):
    """æµ‹è¯•vLLMè¿æ¥"""
    print(f"\nğŸ” æµ‹è¯•vLLMè¿æ¥...")
    print(f"ğŸŒ æœåŠ¡åœ°å€: {base_url}")
    
    try:
        import requests
        
        # 1. æµ‹è¯•å¥åº·æ£€æŸ¥
        print("1ï¸âƒ£ æ£€æŸ¥æœåŠ¡çŠ¶æ€...")
        try:
            health_response = requests.get(f"{base_url}/health", timeout=5)
            if health_response.status_code == 200:
                print("âœ… æœåŠ¡å¥åº·çŠ¶æ€æ­£å¸¸")
            else:
                print(f"âš ï¸  æœåŠ¡çŠ¶æ€å¼‚å¸¸: {health_response.status_code}")
        except:
            print("âš ï¸  æ— æ³•è®¿é—®å¥åº·æ£€æŸ¥ç«¯ç‚¹")
        
        # 2. è·å–æ¨¡å‹åˆ—è¡¨
        print("2ï¸âƒ£ è·å–å¯ç”¨æ¨¡å‹...")
        models_response = requests.get(f"{base_url}/v1/models", timeout=5)
        models_response.raise_for_status()
        
        models_data = models_response.json()
        if 'data' in models_data and models_data['data']:
            available_models = [model['id'] for model in models_data['data']]
            print(f"âœ… å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
            for model in available_models:
                print(f"   - {model}")
            
            # é€‰æ‹©æµ‹è¯•æ¨¡å‹
            test_model = model_name if model_name else available_models[0]
            print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•: {test_model}")
        else:
            print("âš ï¸  æœªå‘ç°å¯ç”¨æ¨¡å‹")
            test_model = model_name or "unknown"
        
        # 3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("3ï¸âƒ£ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        
        chat_data = {
            "model": test_model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
            ],
            "max_tokens": 50,
            "temperature": 0.7
        }
        
        chat_response = requests.post(
            f"{base_url}/v1/chat/completions",
            json=chat_data,
            timeout=30
        )
        chat_response.raise_for_status()
        
        chat_result = chat_response.json()
        if 'choices' in chat_result and chat_result['choices']:
            generated_text = chat_result['choices'][0]['message']['content']
            print("âœ… æ–‡æœ¬ç”Ÿæˆæµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“ ç”Ÿæˆå†…å®¹: {generated_text[:100]}...")
            
            # æ˜¾ç¤ºæ€§èƒ½ä¿¡æ¯
            if 'usage' in chat_result:
                usage = chat_result['usage']
                print(f"ğŸ“Š Tokenä½¿ç”¨: {usage.get('total_tokens', 'N/A')}")
        else:
            print("âŒ æ–‡æœ¬ç”Ÿæˆå“åº”æ ¼å¼å¼‚å¸¸")
        
        print("\nğŸ‰ vLLMè¿æ¥æµ‹è¯•å®Œæˆ!")
        print("âœ… æ‰€æœ‰æµ‹è¯•é¡¹ç›®é€šè¿‡ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ°vLLMæœåŠ¡")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   - vLLMæœåŠ¡æ˜¯å¦å·²å¯åŠ¨")
        print("   - æœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡®")
        print("   - é˜²ç«å¢™è®¾ç½®")
        print("   - ç½‘ç»œè¿æ¥")
        print(f"\nğŸš€ å¯åŠ¨vLLMæœåŠ¡:")
        print(f"   ./start_vllm.sh")
        print(f"   æˆ–æŸ¥çœ‹è¯¦ç»†æŒ‡å—: VLLM_SETUP_GUIDE.md")
        
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶: vLLMæœåŠ¡å“åº”ç¼“æ…¢")
        print("ğŸ’¡ å»ºè®®:")
        print("   - æ£€æŸ¥æœåŠ¡å™¨è´Ÿè½½")
        print("   - å°è¯•å¢åŠ è¶…æ—¶æ—¶é—´")
        print("   - æ£€æŸ¥æ¨¡å‹æ˜¯å¦å®Œå…¨åŠ è½½")
        
    except Exception as e:
        print(f"âŒ vLLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   - vLLMæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        print("   - APIæ¥å£æ˜¯å¦å…¼å®¹OpenAIæ ¼å¼")
        print("   - è¯·æ±‚æ ¼å¼æ˜¯å¦æ­£ç¡®")

def write_env_config(config):
    """å†™å…¥ç¯å¢ƒé…ç½®"""
    env_file = Path('.env')
    
    # è¯»å–ç°æœ‰é…ç½®
    existing_config = {}
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                if '=' in line and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    existing_config[key] = value
    
    # æ›´æ–°é…ç½®
    existing_config.update(config)
    
    # å†™å…¥é…ç½®
    with open(env_file, 'w', encoding='utf-8') as f:
        f.write("# å¤šæ™ºèƒ½ä½“æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿé…ç½®æ–‡ä»¶\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {os.popen('date').read().strip()}\n\n")
        
        for key, value in existing_config.items():
            f.write(f"{key}={value}\n")
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ° {env_file}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ é…ç½®å·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ é…ç½®è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
